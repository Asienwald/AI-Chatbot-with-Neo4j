{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='cyan'>Chatbot API with Keras Model</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main challenges:\n",
    "- Classify user input to recognise intent\n",
    "- keep context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras deep learning library to build classification model\n",
    "# Lancaster stemming library used to collapse distinct word forms\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# things we need for Tensorflow\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Chatbot intents & patterns to learn defined in a plain JSON file\n",
    "Classification model can be created for small vocabulary\n",
    "Need to build vocabulary > patterns processed\n",
    "each word stemmed to produce generic root > help cover more combi for inputs\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('chat_intents.json') as intent_file:\n",
    "    intents = json.load(intent_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?']\n",
    "\n",
    "#loop through each sentence in our intents patterns\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        # tokenize each word in sentence\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        \n",
    "        # add to words list\n",
    "        words.extend(w)\n",
    "        \n",
    "        # add documents in corpus\n",
    "        documents.append((w, intent['tag']))\n",
    "        \n",
    "        # add to our classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "# stem & lower ea word & remove dupes\n",
    "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "# sort classes\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "# documents = combination between patterns & intents\n",
    "print(len(documents), 'documents', documents)\n",
    "# classes = intents\n",
    "print(len(classes), \"classes\", classes)\n",
    "# words = all words, vocab\n",
    "print(len(words), \"unique stemmed words\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Training won't be run based on vocab of words (words meaningless for machine)\n",
    "Need to translate words into bags of words with arr containing 0/1\n",
    "\n",
    "Arr length will be equal to vocab size & 1 set when word from current\n",
    "pattern is located in a given position\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our training data\n",
    "training = []\n",
    "# create an empty array for our output\n",
    "output_empty = [0] * len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set, bag of words for ea sentence\n",
    "for doc in documents:\n",
    "    # initialise our bag of words\n",
    "    bag = []\n",
    "    #list tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    # stem ea word - create base word to represent related words\n",
    "    # Please read stemming VS lemmatization\n",
    "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
    "    \n",
    "    # create our bag of words arr with 1, if word match found in\n",
    "    # current pattern\n",
    "    length = 0\n",
    "    for w in words:\n",
    "        # bag.append(1) if w in pattern_words else bag.append(0)\n",
    "        if w in pattern_words:\n",
    "          bag.append(1)\n",
    "          length += 1\n",
    "        else:\n",
    "          bag.append(0)\n",
    "        \n",
    "    # output is '0' for ea tag & '1' for current tag (for ea pattern)\n",
    "    # output_row is basically the class/intents it falls in\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    \n",
    "    # append length of sentence into training data\n",
    "    # bag.append(length)\n",
    "    \n",
    "    training.append([bag, output_row])\n",
    "    \n",
    "    # print(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle our features & turn into np.array\n",
    "random.shuffle(training)\n",
    "training = np.array(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train & test lists. X - patterns, Y - intents\n",
    "# training_x = [training[:,0], training[:,2]]\n",
    "train_x = list(training[:,0])\n",
    "train_y = list(training[:,1])\n",
    "print(train_x)\n",
    "print(len(train_x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('model.pickle', 'wb') as f:\n",
    "#  pickle.dump((words, labels, training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Training data - X (pattern converted into array [0,1,0,1...,0])\n",
    "Y (intents converted into arr [1,0,0,0,..,0]), will be single 1 for intents arr\n",
    "\n",
    "Model built on 3 layers\n",
    "\n",
    "Classification output will be multiclass arr > help identify encoded intent\n",
    "\n",
    "Use Softmax activation to produce multiclass classification output\n",
    "(result returns arr of 0/1: [1,0,0...,0] - identifies encoded intent)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>**NOTE</font> <br>\n",
    "Ok so Cael & Joe, <br>\n",
    "This part below is basically what makes up a Neural network in ML\n",
    "\n",
    "So basically a neural network has different layers in its hidden layer (main layer)\n",
    "that tries to sift out features to identify what is going on and learn\n",
    "\n",
    "In each layer we have number of neurons to help us know what's going on <br>\n",
    "> Simple Eg. When you touch something you have nerves to tell you if you really touched something or not\n",
    "\n",
    "Softmax is the activation function we're using to determine when to trigger these neurons <br>\n",
    "> Eg. Did you really touch something?? How do you know? > Your nerves felt something and sent signals to your brain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input shape = specify shape of your data its dealing with in 1st layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model - 3 layers\n",
    "# 1st layer 128 neurons, 2nd layer 64 neurons & 3rd contains num of neurons\n",
    "# equal to num of intents to predict output intent with softmax\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape = (len(train_x[0]), ), activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_y[0]), activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Keras model with SGD optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model. Stochastic gradient descent with Nesterov accelerated\n",
    "# gradient gives good results for this model\n",
    "\n",
    "sgd = SGD(lr = 0.01, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "# print(dir(sgd))\n",
    "# print(sgd.lr)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = sgd,\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute training & construct classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "# Epochs means iterations\n",
    "model.fit(np.array(train_x), np.array(train_y), epochs = 200,\n",
    "          batch_size = 5, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_sentence(sentence):\n",
    "    # tokenize the pattern - split words into arr\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    # stem ea word - create short form for word\n",
    "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "    return sentence_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate user sentence into bag of words with arr 0/1\n",
    "# 0 or 1 for ea word in bag that exists in the sentence\n",
    "def bow(sentence, words, show_details = True):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    # Initialise bag of words - matrix of N words, vocab matrix\n",
    "    bag = [0] * len(words)\n",
    "    \n",
    "    for s in sentence_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == s:\n",
    "                # assign 1 if current word in vocab position\n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print(f\"Found in bag: {w}\")\n",
    "                    \n",
    "    # bag.append(len(sentence_words))\n",
    "    return np.array(bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example translating sentence into bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = bow(\"what is cca\", words)\n",
    "print(p)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Good practice to save trained model into pickle file to reuse to publish\n",
    "through Flask REST API\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pickle to load in pre-trained model\n",
    "# global graph\n",
    "# graph = tf.get_default_graph()\n",
    "\n",
    "# with open(f\"katana-assistant-model.pkl\", 'rb') as f:\n",
    "#     model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "before publishing model through Flask REST API, always good to run extra test\n",
    "use model.predict to classify user input & based on calculated\n",
    "probability return intent (multiple intents can be returned)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_local(sentence):\n",
    "    ERROR_THRESHOLD = 0.25\n",
    "    \n",
    "    # generate probabilities from model\n",
    "    input_data = pd.DataFrame([bow(sentence, words)], dtype = float,\n",
    "                              index = ['input'])\n",
    "    print([input_data])\n",
    "    \n",
    "    results = model.predict([input_data])[0]\n",
    "    print(results)\n",
    "    # filter predictions below a threshold & provide intent index\n",
    "    \n",
    "#     for i, r in enumerate(results):\n",
    "#       print(i)\n",
    "#       print(r)\n",
    "#       print(r > ERROR_THRESHOLD)\n",
    "#       print()\n",
    "    \n",
    "    results = [[i, r] for i, r in enumerate(results) if r > ERROR_THRESHOLD]\n",
    "    # sort by strength of probability\n",
    "    results.sort(key = lambda x: x[1], reverse = True)\n",
    "    \n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        # print(r)\n",
    "        return_list.append((classes[r[0]], str(r[1])))\n",
    "    # return tuple of intent & probability\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_local('how to create a cca')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publish same function through REST endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
