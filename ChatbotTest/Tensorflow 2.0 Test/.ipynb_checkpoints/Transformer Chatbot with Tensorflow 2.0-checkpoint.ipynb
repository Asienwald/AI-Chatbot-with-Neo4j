{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='cyan'>Transformer Chatbot with Tensorflow 2.0</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://medium.com/tensorflow/a-transformer-chatbot-tutorial-with-tensorflow-2-0-88bf59e66fe2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Transformer Models\n",
    "- handles variable-sized input using stacks of self-attention layers\n",
    "  instead of RNNs/CNNs\n",
    "## Advantages\n",
    "- makes no assumptions about temporal/spatial r/s across data\n",
    "- layer outputs calculated in parallel instead of series like RNN\n",
    "- distant items can affect ea other's output w/o passing through many\n",
    "  recurrent steps/convolution layers\n",
    "- can learn long-range dependencies\n",
    "\n",
    "## Disadvantages\n",
    "- output of time step calculated from entire history instead of only\n",
    "  inputs & current hidden-state (less efficient)\n",
    "- if input no temporal/spatial r/s like text, some positional\n",
    "  encoding must be added or model will see bag of words\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBuild input pipeline with following steps\\n1. Extract list of conversation pairs from move_conversartions.txt\\n& movie_lines.txt\\n2. Preprocess ea sentence by removing special chars in ea sentence\\n3. Build tokenizer (map text ID & ID to text) with Tensorflow Datasets\\nSubwordTextEncoder\\n4. Tokenize ea sentence & add START_TOKEN & END_TOKEN to indicate start & end\\nof ea sentence\\n5. Filter out ea sentences that contain more than MAX_LENGTH tokens\\n6. Pad tokenized sentences to MAX_LENGTH\\n7. Build tf.data.Dataset with tokenized sentences\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Build input pipeline with following steps\n",
    "1. Extract list of conversation pairs from move_conversartions.txt\n",
    "& movie_lines.txt\n",
    "2. Preprocess ea sentence by removing special chars in ea sentence\n",
    "3. Build tokenizer (map text ID & ID to text) with Tensorflow Datasets\n",
    "SubwordTextEncoder\n",
    "4. Tokenize ea sentence & add START_TOKEN & END_TOKEN to indicate start & end\n",
    "of ea sentence\n",
    "5. Filter out ea sentences that contain more than MAX_LENGTH tokens\n",
    "6. Pad tokenized sentences to MAX_LENGTH\n",
    "7. Build tf.data.Dataset with tokenized sentences\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Transformer is autoaggressive model\n",
    "- makes predictions 1 part at a time & uses output to decide\n",
    "  what to do next\n",
    "This example uses teacher-forcing\n",
    "- Passing true output to next time step regardless of what model\n",
    "  predicts at current time step\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Attention Layers\n",
    "- Transformer consists of encoder & decoder like many seq to seq models\n",
    "- Instead of recurrent/convolution layers, Transformer uses multi-head\n",
    "  attention layers which consists of multiple scaled dot-product\n",
    "  attention\n",
    "  \n",
    "Scaled dot product attention\n",
    "- takes 3 inputs: Q (query), K (key), V (value)\n",
    "- softmax normalisation applied to key, its values decide the amt \n",
    "  of importance given to the query\n",
    "- the output represents the multiplication of attention weights\n",
    "  & value\n",
    "- ensures words we want to focus on kept as is & irrelevant words flushed outb\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm confused :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
